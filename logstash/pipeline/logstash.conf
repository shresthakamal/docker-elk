input {
	beats {
		port => 5044
	}

	tcp {
		port => 50000
	}
}

## Add your filters / logstash plugins configuration here
filter {
	# Apache Access Log Processing
	if [log_type] == "apache_access" {
		grok {
			match => { 
				"message" => "%{COMBINEDAPACHELOG}" 
			}
		}
		
		# Create your custom fields
		mutate {
			add_field => { 
				"request_date" => "%{timestamp}"
				"request_method_url" => "%{verb} %{request}"
				"response_status" => "%{response}"
			}
		}
		
		# Parse the timestamp to proper date format
		date {
			match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
		}
		
		# Clean up - remove fields we don't need
		mutate {
			remove_field => [ "timestamp", "verb", "request", "response", "httpversion", "rawrequest" ]
		}
	}

	# Parse Docker container logs (existing functionality)
	if [container][name] {
		mutate {
			add_field => { "container_name" => "%{[container][name]}" }
		}
	}

	# Add timestamp parsing for better time handling
	if [log][file][path] {
		mutate {
			add_field => { "log_source" => "%{[log][file][path]}" }
		}
	}

	# Parse JSON logs if they exist
	if [message] =~ /^\{.*\}$/ {
		json {
			source => "message"
		}
	}
}

output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "logstash_internal"
		password => "${LOGSTASH_INTERNAL_PASSWORD}"
		# Create index pattern based on input source
		index => "filebeat-%{+YYYY.MM.dd}"
		# Allow Logstash to manage index templates and creation
		manage_template => true
		template_overwrite => true
	}

	# Optional: Output to stdout for debugging
	# stdout { codec => rubydebug }
}
